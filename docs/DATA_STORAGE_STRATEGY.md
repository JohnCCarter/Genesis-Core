# Data Storage Strategy - Genesis-Core

## ğŸ“¦ Format Philosophy

**Hybrid approach: Right format for right use case**

### Feather vs Parquet - When to use what?

| Format | Use Case | Why | Speed | Compression |
|--------|----------|-----|-------|-------------|
| **Feather** | Features (frequent reads) | 2Ã— faster reads | âš¡âš¡âš¡ ~20ms | Medium |
| **Parquet** | Raw candles (long-term storage) | Better compression, inter-op | âš¡âš¡ ~40ms | High |

---

## ğŸ—ï¸ Current Architecture

```
data/
â”œâ”€â”€ candles/                    # âŒ OLD: Flat structure
â”‚   â””â”€â”€ {symbol}_{timeframe}.parquet
â”‚
â””â”€â”€ features/                   # âœ… GOOD: Hybrid format
    â”œâ”€â”€ {symbol}_{timeframe}_features_v17.feather  # Primary (fast)
    â””â”€â”€ {symbol}_{timeframe}_features_v17.parquet  # Backup
```

### Smart loader (src/core/utils/data_loader.py):
```python
# Try Feather first (2Ã— faster)
if feather_path.exists():
    return pd.read_feather(feather_path)
# Fallback to Parquet
elif parquet_path.exists():
    return pd.read_parquet(parquet_path)
```

---

## ğŸ¯ Proposed: Two-Layer Architecture

```
data/
â”œâ”€â”€ raw/                        # Raw Lake (immutable, timestamped)
â”‚   â””â”€â”€ bitfinex/
â”‚       â””â”€â”€ candles/
â”‚           â””â”€â”€ {symbol}_{timeframe}_{YYYY-MM-DD}.parquet
â”‚
â”œâ”€â”€ curated/                    # Gold Layer (validated, versioned)
â”‚   â””â”€â”€ v1/
â”‚       â”œâ”€â”€ candles/
â”‚       â”‚   â””â”€â”€ {symbol}_{timeframe}.parquet
â”‚       â””â”€â”€ features/
â”‚           â”œâ”€â”€ {symbol}_{timeframe}_features.feather  # Primary
â”‚           â””â”€â”€ {symbol}_{timeframe}_features.parquet  # Backup
â”‚
â””â”€â”€ metadata/
    â””â”€â”€ curated/
        â””â”€â”€ {symbol}_{timeframe}_v1.json
```

---

## ğŸ“Š Format Performance Comparison

### Read Performance (tBTCUSD_1h, 12,960 bars):

| Format | Read Time | Use Case |
|--------|-----------|----------|
| Feather | ~20ms | âœ… Features (read frequently during training) |
| Parquet | ~40ms | âœ… Candles (read once, compress well) |
| CSV | ~150ms | âŒ Legacy only |

### Storage (18 months data):

| Data Type | Feather | Parquet | CSV |
|-----------|---------|---------|-----|
| Candles (OHLCV) | ~100 KB | ~75 KB â­ | ~500 KB |
| Features (17 cols) | ~250 KB â­ | ~200 KB | ~1.2 MB |

**Conclusion:**
- Candles: Parquet saves ~25% space, read speed less critical
- Features: Feather 2Ã— faster, minimal size penalty

---

## ğŸ”„ Migration Strategy

### Step 1: Restructure (dry-run first)
```bash
# See what would change
python scripts/restructure_data_layers.py --dry-run

# Execute migration
python scripts/restructure_data_layers.py --execute
```

### Step 2: Update fetch scripts
```python
# OLD: Overwrites existing
fetch_historical.py â†’ data/candles/{symbol}_{timeframe}.parquet

# NEW: Timestamped + versioned
fetch_historical.py â†’ data/raw/bitfinex/candles/{symbol}_{timeframe}_{date}.parquet
                   â†’ data/curated/v1/candles/{symbol}_{timeframe}.parquet
```

### Step 3: Incremental sync (future)
```python
# Only fetch new data since last sync
sync_incremental.py â†’ Append to Raw Lake
                   â†’ Update Curated with validation
```

---

## ğŸ“ Best Practices

### For Candles (OHLCV):
```python
# âœ… GOOD: Parquet with snappy compression
df.to_parquet(path, index=False, compression="snappy")

# âŒ BAD: Feather for long-term storage (less compression)
df.to_feather(path)  # Use only for features!
```

### For Features:
```python
# âœ… BEST: Both formats for flexibility
df.to_feather(f"{symbol}_{tf}_features_v17.feather")  # Fast reads
df.to_parquet(f"{symbol}_{tf}_features_v17.parquet")  # Backup

# Loader tries Feather first, falls back to Parquet
```

### For Large Datasets (future):
```python
# Partition by year/month for massive datasets
df.to_parquet(
    "data/raw/bitfinex/candles",
    partition_cols=["year", "month"],
    compression="snappy"
)
```

---

## ğŸ“ Why This Matters

### Problem 1: Concept Drift
**Without versioning:**
```
2024-10-01: Fetch data â†’ Train model â†’ IC = 0.05
2024-10-15: Re-fetch data (different endpoint/filter) â†’ IC = 0.03
```
**What changed?** No way to know!

**With versioning:**
```
data/raw/bitfinex/candles/tBTCUSD_1h_2024-10-01.parquet  (immutable)
data/curated/v1/candles/tBTCUSD_1h.parquet  (validated)
data/metadata/curated/tBTCUSD_1h_v1.json  (quality_score: 0.98)
```
**Reproducible!** Can always trace back.

### Problem 2: Incremental Updates
**Without Raw Lake:**
```
Daily sync overwrites existing data
â†’ Can't verify if new data matches old data
â†’ Risk of introducing gaps/inconsistencies
```

**With Raw Lake:**
```
Daily sync â†’ Append to Raw Lake (timestamped)
           â†’ Validate + merge into Curated
           â†’ Track adjustments in metadata
```

### Problem 3: Multi-Exchange (future)
**Genesis-Core might expand to:**
```
data/raw/
â”œâ”€â”€ bitfinex/
â”œâ”€â”€ binance/
â””â”€â”€ kraken/

data/curated/v1/  (merged, normalized, validated)
```

---

## ğŸš€ Implementation Priority

### Phase 1: Immediate (this week)
- [x] Document current format strategy
- [x] Create restructure script with dry-run
- [ ] Test migration on existing data
- [ ] Update fetch scripts to use two-layer

### Phase 2: Short-term (next sprint)
- [ ] Implement incremental sync
- [ ] Add quality monitoring
- [ ] Version bump process (v1 â†’ v2)

### Phase 3: Long-term (future)
- [ ] Multi-exchange support
- [ ] Automated data quality checks
- [ ] Cloud backup strategy

---

## ğŸ“š References

- **Feather format:** https://arrow.apache.org/docs/python/feather.html
- **Parquet format:** https://parquet.apache.org/
- **Genesis-Core data loader:** `src/core/utils/data_loader.py`
- **Migration script:** `scripts/restructure_data_layers.py`

---

**Updated:** 2025-10-10  
**Author:** Genesis-Core Team  
**Status:** Proposed â†’ Pending Review

