"""
Train meta-labeling model for FVG trading.

The meta-model is a binary classifier that decides ACCEPT/REJECT
for each FVG signal generated by the primary filter.

This improves Profit Factor by filtering out losing trades while
keeping winning trades.
"""

import argparse
import json
import sys
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    precision_score,
    recall_score,
    roc_auc_score,
)
from sklearn.model_selection import GridSearchCV

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))


def train_meta_classifier(
    features_df: pd.DataFrame,
    meta_labels_df: pd.DataFrame,
    test_size: float = 0.2,
) -> tuple[dict, dict]:
    """
    Train binary classifier for meta-labeling.

    Args:
        features_df: All features for each bar
        meta_labels_df: Meta-labels (index, signal, meta_label)
        test_size: Fraction for test set

    Returns:
        (model_dict, metrics_dict)
    """
    print("[PREPARE] Merging features with meta-labels...")

    # Get indices where we have FVG signals
    signal_indices = meta_labels_df["index"].values

    # Extract features for these indices
    X = features_df.loc[signal_indices].copy()
    y = meta_labels_df["meta_label"].values

    # Remove non-numeric columns (like timestamp)
    numeric_cols = X.select_dtypes(include=[np.number]).columns
    X = X[numeric_cols]

    print(f"[CLEAN] Using {len(numeric_cols)} numeric features")

    # Remove NaN
    nan_mask = X.isna().any(axis=1)
    if nan_mask.any():
        print(f"[CLEAN] Removing {nan_mask.sum()} rows with NaN")
        X = X[~nan_mask]
        y = y[~nan_mask]

    print(f"[DATA] Training samples: {len(X)}")
    print(f"[DATA] Features: {len(X.columns)}")
    print(f"[DATA] Accept rate: {y.mean():.1%}")

    # Chronological split
    split_idx = int(len(X) * (1 - test_size))
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    print(f"[SPLIT] Train: {len(X_train)}, Test: {len(X_test)}")

    # Train with GridSearchCV
    print("[TRAIN] Training meta-classifier...")

    param_grid = {
        "C": [0.01, 0.1, 1.0, 10.0],
        "class_weight": ["balanced", None],
        "solver": ["lbfgs"],
        "max_iter": [1000],
    }

    clf = GridSearchCV(
        LogisticRegression(random_state=42),
        param_grid,
        cv=5,
        scoring="roc_auc",
        n_jobs=-1,
    )

    clf.fit(X_train, y_train)

    print(f"[TRAIN] Best params: {clf.best_params_}")
    print(f"[TRAIN] Best CV AUC: {clf.best_score_:.4f}")

    # Evaluate
    print("[EVAL] Evaluating on test set...")

    y_pred_proba = clf.predict_proba(X_test)[:, 1]
    y_pred = clf.predict(X_test)

    metrics = {
        "auc": roc_auc_score(y_test, y_pred_proba),
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred, zero_division=0),
        "recall": recall_score(y_test, y_pred, zero_division=0),
        "confusion_matrix": confusion_matrix(y_test, y_pred).tolist(),
        "baseline_accept_rate": float(y_train.mean()),
        "test_accept_rate": float(y_test.mean()),
    }

    print(f"\n[METRICS] Test AUC: {metrics['auc']:.4f}")
    print(f"[METRICS] Accuracy: {metrics['accuracy']:.4f}")
    print(f"[METRICS] Precision: {metrics['precision']:.4f}")
    print(f"[METRICS] Recall: {metrics['recall']:.4f}")
    print("[METRICS] Confusion Matrix:")
    print(f"  {metrics['confusion_matrix']}")

    # Convert to Genesis-Core format
    best_model = clf.best_estimator_

    model_dict = {
        "accept": {
            "w": best_model.coef_[0].tolist(),
            "b": float(best_model.intercept_[0]),
        },
        "schema": X.columns.tolist(),
        "meta": {
            "model_type": "meta_classifier",
            "best_params": clf.best_params_,
            "cv_auc": float(clf.best_score_),
        },
    }

    return model_dict, metrics


def main():
    parser = argparse.ArgumentParser(description="Train meta-labeling model")
    parser.add_argument("--symbol", type=str, required=True, help="Symbol (e.g., tBTCUSD)")
    parser.add_argument("--timeframe", type=str, required=True, help="Timeframe (e.g., 1h)")
    parser.add_argument("--test-size", type=float, default=0.2, help="Test set fraction")
    parser.add_argument("--version", type=str, default="v1_meta", help="Model version name")

    args = parser.parse_args()

    # Load data
    features_path = Path(f"data/features/{args.symbol}_{args.timeframe}_features.parquet")
    meta_labels_path = Path(f"data/meta_labels/{args.symbol}_{args.timeframe}_meta_labels.parquet")

    if not features_path.exists():
        print(f"[ERROR] Features not found: {features_path}")
        sys.exit(1)

    if not meta_labels_path.exists():
        print(f"[ERROR] Meta-labels not found: {meta_labels_path}")
        print("[HINT] Run: python scripts/generate_meta_labels.py first")
        sys.exit(1)

    from core.utils.data_loader import load_features

    print("[LOAD] Loading features...")
    features_df = load_features(args.symbol, args.timeframe)

    print(f"[LOAD] {meta_labels_path}")
    meta_labels_df = pd.read_parquet(meta_labels_path)

    # Train
    model_dict, metrics = train_meta_classifier(
        features_df,
        meta_labels_df,
        test_size=args.test_size,
    )

    # Save
    output_dir = Path("results/models")
    output_dir.mkdir(parents=True, exist_ok=True)

    model_path = output_dir / f"{args.symbol}_{args.timeframe}_{args.version}.json"
    metrics_path = output_dir / f"{args.symbol}_{args.timeframe}_{args.version}_metrics.json"

    with open(model_path, "w") as f:
        json.dump(model_dict, f, indent=2)

    with open(metrics_path, "w") as f:
        json.dump(metrics, f, indent=2)

    print(f"\n[SUCCESS] Model saved: {model_path}")
    print(f"[SUCCESS] Metrics saved: {metrics_path}")


if __name__ == "__main__":
    main()
